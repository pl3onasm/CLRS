$\huge{\color{cadetblue} \text{Quicksort}}$

<br/>

Quicksort is a comparison sort based on the ${\color{darkseagreen} \text{divide and conquer}}$ paradigm. It divides the original problem into subproblems of the same type by selecting a ${\color{darkseagreen} \text{pivot element}}$ around which the array is then partitioned. The result of this partitioning is that the pivot is in its final sorted position, and we are left with two subarrays: one with all elements less than the pivot and another with all elements greater than the pivot. These subarrays are then recursively sorted using the same process. The base case is when the subarray contains only one element, at which point the entire subarray is trivially sorted. Unlike [merge sort](https://github.com/pl3onasm/CLRS/tree/main/algorithms/sorting/merge-sort), quicksort has no need for a combine step in the form of a merge procedure, since the partitioning step ensures that in the end all the elements are in their final sorted positions. Quicksort is an ${\color{darkseagreen} \text{unstable}}$ and ${\color{darkseagreen} \text{in-place}}$ sort, meaning that it does not preserve the relative order of elements with equal values, and it does not require any additional memory.

Clearly, the ${\color{darkseagreen} \text{partitioning}}$  is the crucial step of the algorithm. This is done by maintaining a single pointer which points to the dividing line between the elements less than the pivot and those greater than the pivot. This pointer is initialized to the first element of the array. Then, for each element in the array, if the element is less than the pivot, the element is swapped with the element at the pointer and the pointer is incremented. This ensures that all elements less than the pivot are to the left of the pointer, and all elements greater than the pivot are to the right of said pointer. Finally, the pivot is swapped with the element at the pointer, placing it in its final sorted position. The pointer is returned, and quick sort is then recursively called on the subarrays to the left and right of the pivot. Once the base case is reached, the input array is sorted.

Much of quicksort's performance depends on the ${\color{darkseagreen} \text{pivot se} \text{lection}}$, since this determines how ${\color{darkseagreen} \text{balanced}}$ the partitioning is, i.e. how evenly the array is divided, and thus how many recursive calls are made. If the pivot is chosen to be the last element of the array, and the array is already sorted, then the partitioning will result in one subarray of size $1$ and one subarray of size $n-1$, where $n$ is the size of the original array. It is quite ironic that an already sorted array thus elicits quicksort's worst case time complexity of $\mathcal{O}(n^2)$. The average case time complexity, however, is ${\color{rosybrown}\mathcal{O}(n \log n)}$, which is the same as merge sort's time complexity. This is because the partitioning takes $\mathcal{O}(n)$ time, and results, in the ideal case, in two subarrays of size $n/2$ (this would be the case if the pivot were the median element of the array), which are then recursively sorted in $\log n$ recursive calls.

Implementation: [Quicksort](https://github.com/pl3onasm/CLRS/tree/main/algorithms/sorting/quick-sort/quicksort.c)

In order to avoid the worst case time complexity, the pivot can be chosen ${\color{darkseagreen} \text{randomly}}$ at each step. This ensures that the partitioning is balanced on average, and thus the average case time complexity is maintained. The worst case time complexity is still $\mathcal{O}(n^2)$, but the probability of this occurring is negligible. We say that the algorithm runs in expected $\mathcal{O}(n \log n)$ time.

Implementation: [Randomized Quicksort](https://github.com/pl3onasm/CLRS/tree/main/algorithms/sorting/quick-sort/randomqsort.c)
